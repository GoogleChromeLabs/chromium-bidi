# This workflow will run measure performance in different environments.

name: Run performance benchmarks

# Declare default permissions as read only.
permissions: read-all

env:
  DEBUG: 'bidi:server:*,bidi:mapper:*'
  DEBUG_DEPTH: 10
  FORCE_COLOR: 3
  PIP_DISABLE_PIP_VERSION_CHECK: 1

on:
  merge_group:
  pull_request:
  push:
    branches: 'main'
  workflow_dispatch:
    inputs:
      verbose:
        description: Verbose logging
        default: false
        required: false
        type: boolean

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  pup-perf-metric:
    name: Puppeteer ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        # TODO(#876): Add Windows CI.
        os: [ubuntu-latest, macos-latest]
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout
        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
      - name: Set up Node.js
        uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6.2.0
        with:
          node-version-file: '.nvmrc'
          cache: npm
      - name: Disable AppArmor
        if: ${{ matrix.os == 'ubuntu-latest' }}
        # https://chromium.googlesource.com/chromium/src/+/main/docs/security/apparmor-userns-restrictions.md
        run: echo 0 | sudo tee /proc/sys/kernel/apparmor_restrict_unprivileged_userns
      - uses: google/wireit@f21db1f3a6a4db31f42787a958cf2a18308effed # setup-github-actions-caching/v2.0.3
      - name: Install and build npm dependencies
        run: npm ci
      # Install chrome, chromedriver and headless shell is required to keep them cached.
      - name: Install all chrome binaries if needed
        uses: ./.github/actions/setup-chrome-binaries
      - name: Build chromium-bidi
        run: npm run build
      - name: Install Puppeteer
        # Use specific Puppeteer version for result consistency.
        run: npm install puppeteer@24.36.0
      - name: Link chromium-bidi
        run: |
          npm link
          npm link chromium-bidi
      - name: Run JS performance benchmark
        run: |
          node tools/benchmark-puppeteer.mjs | tee -a tests_output.txt
      - name: Extract and store performance metrics
        id: extract_metrics
        # Publish 2 metrics: mean and median relative difference. Group them together by
        # providing the same `extra` field.
        run: |
          echo "" > pup_perf_metrics.json

          mean_value=$(grep 'PERF_METRIC:DIFF_MEAN_REL:VALUE:' tests_output.txt | sed 's/^PERF_METRIC:DIFF_MEAN_REL:VALUE://')
          mean_range=$(grep 'PERF_METRIC:DIFF_MEAN_REL:RANGE:' tests_output.txt | sed 's/^PERF_METRIC:DIFF_MEAN_REL:RANGE://')
          echo "{\"name\": \"${{ matrix.os }}:pup-perf-metric:diff_mean_rel\", \"value\": $mean_value, \"range\": $mean_range, \"unit\": \"Percent\", \"group\": \"pup-perf-metric\", \"extra\": \"${{ matrix.os }}:pup-perf-metric:diff_rel\"}," >> pup_perf_metrics.json

          median_value=$(grep 'PERF_METRIC:DIFF_MEDIAN_REL:VALUE:' tests_output.txt | sed 's/^PERF_METRIC:DIFF_MEDIAN_REL:VALUE://')
          median_range=$(grep 'PERF_METRIC:DIFF_MEDIAN_REL:RANGE:' tests_output.txt | sed 's/^PERF_METRIC:DIFF_MEDIAN_REL:RANGE://')
          echo "{\"name\": \"${{ matrix.os }}:pup-perf-metric:diff_median_rel\", \"value\": $median_value, \"range\": $median_range, \"unit\": \"Percent\", \"group\": \"pup-perf-metric\", \"extra\": \"${{ matrix.os }}:pup-perf-metric:diff_rel\"}" >> pup_perf_metrics.json
          echo "Extracted performance metrics:"
          cat pup_perf_metrics.json
      - name: Upload performance result as artifact
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: pup-perf-metrics-${{ matrix.os }}
          path: pup_perf_metrics.json
          # Do not store these artifacts for long, as they will be present in the `all-performance-metrics`.
          retention-days: 1
  e2e-perf-metric:
    name: E2e ${{ matrix.kind }}-${{ matrix.os }}-${{ matrix.head }}
    strategy:
      fail-fast: false
      matrix:
        # TODO(#876): Add Windows CI.
        os: [ubuntu-latest, macos-latest]
        head: [headful, 'new-headless', 'old-headless']
        # `cd` runs e2e via `chromedriver`. `node` runs tests using `NodeJS` runner.
        kind: [cd, node]
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout
        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
      - name: Set up Node.js
        uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6.2.0
        with:
          node-version-file: '.nvmrc'
          cache: npm
      - name: Disable AppArmor
        if: ${{ matrix.os == 'ubuntu-latest' }}
        # https://chromium.googlesource.com/chromium/src/+/main/docs/security/apparmor-userns-restrictions.md
        run: echo 0 | sudo tee /proc/sys/kernel/apparmor_restrict_unprivileged_userns
      - uses: google/wireit@f21db1f3a6a4db31f42787a958cf2a18308effed # setup-github-actions-caching/v2.0.3
      - name: Install and build npm dependencies
        run: npm ci
      # Install chrome, chromedriver and headless shell is required to keep them cached.
      - name: Install all chrome binaries if needed
        uses: ./.github/actions/setup-chrome-binaries
      - name: Set up Python
        uses: actions/setup-python@a309ff8b426b58ec0e2a45f0f869d46889d02405 # v6.2.0
        with:
          python-version: '3.11'
          cache: pipenv
      - name: Install pipenv
        run: pip install pipenv
      - name: Install python dependencies
        run: pipenv install
      - name: Run E2E performance tests with xvfb-run
        if: matrix.os == 'ubuntu-latest' && matrix.head == 'headful'
        timeout-minutes: 20
        run: >
          xvfb-run --auto-servernum
          npm run e2e:${{ matrix.head }}
          --
          -k test_performance_
          -s
          | tee tests_output.txt
        env:
          VERBOSE: ${{ github.event.inputs.verbose }}
          CHROMEDRIVER: ${{ matrix.kind == 'cd' }}
      - name: Run E2E performance tests
        if: matrix.os != 'ubuntu-latest' || matrix.head != 'headful'
        timeout-minutes: 20
        run: >
          npm run e2e:${{ matrix.head }}
          --
          -k test_performance_
          -s
          | tee tests_output.txt
        env:
          VERBOSE: ${{ github.event.inputs.verbose }}
          CHROMEDRIVER: ${{ matrix.kind == 'cd' }}
      - name: Extract and store performance metrics
        id: extract_metrics
        env:
          PREFIX: ${{ matrix.os }}-${{ matrix.head }}-${{ matrix.kind }}
        run: |
          echo "" > e2e_perf_metrics.json
          grep 'PERF_METRIC:' tests_output.txt | awk -F'PERF_METRIC:' -v prefix="$PREFIX" '
            {
              split($2, parts, ":");
              if (length(parts) >= 2) {
                if (NR > 1) printf ",";
                printf "{\"name\": \"%s:%s\", \"value\": %s, \"unit\": \"ms\", \"extra\": \"${{ matrix.os }}:e2e-perf-metric\"}", prefix, parts[1], parts[2];
              }
            }
          ' >> e2e_perf_metrics.json
          
          echo "Extracted performance metrics:"
          cat e2e_perf_metrics.json
      - name: Upload performance result as artifact
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: e2e-perf-metrics-${{ matrix.os }}-${{ matrix.head }}-${{ matrix.kind }}
          path: e2e_perf_metrics.json
          # Do not store these artifacts for long, as they will be present in the `all-performance-metrics`.
          retention-days: 1
  combine_metrics:
    if: ${{ !cancelled() }}
    name: Combine performance metrics
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    permissions:
      deployments: write
      # Add perf metrics to PR's comment
      pull-requests: write
      contents: write
    runs-on: ubuntu-latest
    needs: [pup-perf-metric, e2e-perf-metric]
    steps:
      - name: Checkout
        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
      - name: Setup Pages
        uses: actions/configure-pages@983d7736d9b0ae728b81ab479565c72886d7745b # v5.0.0
      - name: Download all performance metrics artifacts
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7.0.0
        with:
          path: artifacts
      - name: Combine metrics into a single file
        run: |
          find artifacts -name "*metrics.json" -print0 | while IFS= read -r -d '' file; do
            if [ "$first_metric" = false ]; then
              echo "," >> performance-metrics-all.json
            fi
            cat "$file" >> performance-metrics-all.json
            first_metric=false
          done

          echo "]" >> performance-metrics-all.json
          echo "Combined performance metrics in JSON format:"
          cat performance-metrics-all.json
      - name: Upload combined performance metrics
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: all-performance-metrics
          path: |
            performance-metrics-all.json
      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'customSmallerIsBetter'
          output-file-path: performance-metrics-all.json
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: bench/
          github-token: ${{ secrets.GITHUB_TOKEN }}
          # Comment only when run in PR.
          comment-always: ${{ github.ref != 'refs/heads/main' }}
          # Push and deploy automatically on merge to main.
          auto-push: ${{ github.ref == 'refs/heads/main' }}
